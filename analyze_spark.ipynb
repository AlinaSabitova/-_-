{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+xK4sTYH7M1pVEfrLovH9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlinaSabitova/-_-/blob/main/analyze_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS2mcSvry6ph"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Анализ зарплат в Data Science с использованием PySpark\n",
        "Задача: найти среднюю зарплату по уровню опыта\n",
        "\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, count, stddev, min as spark_min, max as spark_max, median\n",
        "from pyspark.sql.types import FloatType\n",
        "import sys\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"Создать Spark сессию\"\"\"\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Salary Analysis\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.memory\", \"2g\") \\\n",
        "        .config(\"spark.executor.memory\", \"2g\") \\\n",
        "        .getOrCreate()\n",
        "    return spark\n",
        "\n",
        "def load_data(spark, filepath):\n",
        "    \"\"\"Загрузить данные из HDFS или локального файла\"\"\"\n",
        "    print(f\"Загрузка данных из: {filepath}\")\n",
        "\n",
        "    try:\n",
        "        # Попробовать загрузить из HDFS\n",
        "        df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
        "        print(f\"Данные загружены из HDFS\")\n",
        "    except:\n",
        "        # Если не удалось, загрузить локально\n",
        "        df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
        "        print(f\"Данные загружены локально\")\n",
        "\n",
        "    print(f\"Количество строк: {df.count()}\")\n",
        "    return df\n",
        "\n",
        "def clean_and_prepare(df):\n",
        "    \"\"\"Очистка и подготовка данных\"\"\"\n",
        "    print(\"\\n=== Очистка данных ===\")\n",
        "    print(f\"Исходное количество строк: {df.count()}\")\n",
        "\n",
        "    # Удалить строки с null в зарплате\n",
        "    df = df.filter(col('salary_in_usd').isNotNull())\n",
        "\n",
        "    # Заполнить null в experience_level\n",
        "    df = df.na.fill('Unknown', subset=['experience_level'])\n",
        "\n",
        "    # Создать колонку с полными названиями уровней опыта\n",
        "    from pyspark.sql.functions import when\n",
        "    df = df.withColumn(\n",
        "        'experience_level_name',\n",
        "        when(col('experience_level') == 'EN', 'Entry-level')\n",
        "        .when(col('experience_level') == 'MI', 'Mid-level')\n",
        "        .when(col('experience_level') == 'SE', 'Senior-level')\n",
        "        .when(col('experience_level') == 'EX', 'Executive-level')\n",
        "        .otherwise('Unknown')\n",
        "    )\n",
        "\n",
        "    print(f\"Количество строк после очистки: {df.count()}\")\n",
        "    print(f\"Уникальных уровней опыта: {df.select('experience_level').distinct().count()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_salary_by_experience(df):\n",
        "    \"\"\"Анализ средней зарплаты по уровням опыта\"\"\"\n",
        "    print(\"\\n=== Анализ средней зарплаты по уровням опыта ===\")\n",
        "\n",
        "    # Группировка и агрегация\n",
        "    result = df.groupBy('experience_level', 'experience_level_name') \\\n",
        "        .agg(\n",
        "            avg('salary_in_usd').alias('Mean_Salary_USD'),\n",
        "            count('*').alias('Count'),\n",
        "            stddev('salary_in_usd').alias('Std_Deviation'),\n",
        "            spark_min('salary_in_usd').alias('Min_Salary'),\n",
        "            spark_max('salary_in_usd').alias('Max_Salary')\n",
        "        ) \\\n",
        "        .orderBy(col('Mean_Salary_USD').desc())\n",
        "\n",
        "    return result\n",
        "\n",
        "def additional_analysis(df):\n",
        "    \"\"\"Дополнительный анализ данных\"\"\"\n",
        "    print(\"\\n=== Дополнительная статистика ===\")\n",
        "\n",
        "    # Общая статистика по зарплатам\n",
        "    salary_stats = df.agg(\n",
        "        avg('salary_in_usd').alias('avg_salary'),\n",
        "        spark_min('salary_in_usd').alias('min_salary'),\n",
        "        spark_max('salary_in_usd').alias('max_salary')\n",
        "    ).collect()[0]\n",
        "\n",
        "    print(f\"Общая статистика зарплат в USD:\")\n",
        "    print(f\"Средняя зарплата: ${salary_stats['avg_salary']:,.2f}\")\n",
        "    print(f\"Минимальная зарплата: ${salary_stats['min_salary']:,.2f}\")\n",
        "    print(f\"Максимальная зарплата: ${salary_stats['max_salary']:,.2f}\")\n",
        "\n",
        "    # Распределение по уровням опыта\n",
        "    print(f\"\\nРаспределение по уровням опыта:\")\n",
        "    experience_counts = df.groupBy('experience_level_name').agg(count('*').alias('count')).collect()\n",
        "    total_count = df.count()\n",
        "\n",
        "    for row in experience_counts:\n",
        "        percentage = (row['count'] / total_count) * 100\n",
        "        print(f\"  {row['experience_level_name']}: {row['count']} записей ({percentage:.1f}%)\")\n",
        "\n",
        "def main():\n",
        "    # Путь к данным\n",
        "    hdfs_path = \"hdfs://localhost:9000/user/hadoop/input/salary_data.csv\"\n",
        "    local_path = \"/opt/data/salary_data.csv\"\n",
        "\n",
        "    # Создать Spark сессию\n",
        "    spark = create_spark_session()\n",
        "\n",
        "    print(\"=== Анализ зарплат в Data Science с использованием PySpark ===\")\n",
        "\n",
        "    # Показать конфигурацию\n",
        "    print(\"\\n=== Конфигурация Spark ===\")\n",
        "    print(f\"Version: {spark.version}\")\n",
        "    print(f\"Master: {spark.sparkContext.master}\")\n",
        "\n",
        "    # Загрузить данные\n",
        "    try:\n",
        "        df = load_data(spark, hdfs_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Не удалось загрузить из HDFS: {e}\")\n",
        "        print(\"Попытка загрузить локально...\")\n",
        "        df = load_data(spark, local_path)\n",
        "\n",
        "    # Показать схему и первые строки\n",
        "    print(\"\\n=== Схема данных ===\")\n",
        "    df.printSchema()\n",
        "    print(\"\\nПервые 5 строк (ключевые колонки):\")\n",
        "    df.select('work_year', 'experience_level', 'job_title', 'salary_in_usd').show(5)\n",
        "\n",
        "    # Очистка данных\n",
        "    df_clean = clean_and_prepare(df)\n",
        "\n",
        "    # Анализ\n",
        "    result = analyze_salary_by_experience(df_clean)\n",
        "\n",
        "    # Показать результаты\n",
        "    print(\"\\n=== Результаты анализа зарплат ===\")\n",
        "    print(\"\\nУровни опыта по средней зарплате:\")\n",
        "    result.show(truncate=False)\n",
        "\n",
        "    # Найти уровень с максимальной и минимальной зарплатой\n",
        "    result_list = result.collect()\n",
        "    max_salary_row = result_list[0]\n",
        "    min_salary_row = result_list[-1]\n",
        "\n",
        "    print(f\"\\nУровень опыта с максимальной средней зарплатой: '{max_salary_row['experience_level_name']}' ({max_salary_row['experience_level']})\")\n",
        "    print(f\"Средняя максимальная зарплата: ${max_salary_row['Mean_Salary_USD']:,.2f} USD\")\n",
        "    print(f\"Количество специалистов с максимальной зарплатой: {max_salary_row['Count']}\")\n",
        "    print(f\"Диапазон зарплат: ${max_salary_row['Min_Salary']:,.2f} - ${max_salary_row['Max_Salary']:,.2f} USD\")\n",
        "\n",
        "    print(f\"\\nУровень опыта с минимальной средней зарплатой: '{min_salary_row['experience_level_name']}' ({min_salary_row['experience_level']})\")\n",
        "    print(f\"Средняя минимальная зарплата: ${min_salary_row['Mean_Salary_USD']:,.2f} USD\")\n",
        "    print(f\"Количество специалистов с минимальной зарплатой: {min_salary_row['Count']}\")\n",
        "    print(f\"Диапазон зарплат: ${min_salary_row['Min_Salary']:,.2f} - ${min_salary_row['Max_Salary']:,.2f} USD\")\n",
        "\n",
        "    # Дополнительный анализ\n",
        "    additional_analysis(df_clean)\n",
        "\n",
        "    # Сохранить результаты\n",
        "    output_path = \"results/salary_by_experience_spark\"\n",
        "    result.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
        "    print(f\"\\nРезультаты сохранены в: {output_path}\")\n",
        "\n",
        "    # Попробовать сохранить в HDFS\n",
        "    try:\n",
        "        hdfs_output = \"hdfs://localhost:9000/user/hadoop/output/salary_by_experience\"\n",
        "        result.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(hdfs_output)\n",
        "        print(f\"Результаты также сохранены в HDFS: {hdfs_output}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Не удалось сохранить в HDFS: {e}\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}